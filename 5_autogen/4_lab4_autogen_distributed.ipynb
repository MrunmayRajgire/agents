{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 Day 4\n",
    "\n",
    "AutoGen Core - Distributed\n",
    "\n",
    "I'm only going to give a Teaser of this!!\n",
    "\n",
    "Partly because I'm unsure how relevant it is to you. If you'd like me to add more content for this, please do let me know.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ALL_IN_ONE_WORKER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with our Message class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - a host for our distributed runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n",
    "\n",
    "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
    "host.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's reintroduce a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "serper = GoogleSerperAPIWrapper()\n",
    "langchain_serper =Tool(name=\"internet_search\", func=serper.run, description=\"Useful for when you need to search the internet\")\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction1 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons in favor of choosing AutoGen; the pros of AutoGen.\"\n",
    "\n",
    "instruction2 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons against choosing AutoGen; the cons of Autogen.\"\n",
    "\n",
    "judge = \"You must make a decision on whether to use AutoGen for a project. \\\n",
    "Your research team has come up with the following reasons for and against. \\\n",
    "Based purely on the research from your team, please respond with your decision and brief rationale.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And make some Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        #model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gemini-2.5-pro\", model_info={\n",
    "            \"vision\": True,\n",
    "            \"function_calling\": True,\n",
    "            \"json_output\": True,\n",
    "            \"structured_output\": True,\n",
    "            \"family\":\"gemini-2.5-pro\"\n",
    "        })\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        #model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gemini-2.5-pro\", model_info={\n",
    "            \"vision\": True,\n",
    "            \"function_calling\": True,\n",
    "            \"json_output\": True,\n",
    "            \"structured_output\": True,\n",
    "            \"family\":\"gemini-2.5-pro\"\n",
    "        })\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Judge(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        #model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gemini-2.5-pro\", model_info={\n",
    "            \"vision\": True,\n",
    "            \"function_calling\": True,\n",
    "            \"json_output\": True,\n",
    "            \"structured_output\": True,\n",
    "            \"family\":\"gemini-2.5-pro\"\n",
    "        })\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        message1 = Message(content=instruction1)\n",
    "        message2 = Message(content=instruction2)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message1, inner_1)\n",
    "        response2 = await self.send_message(message2, inner_2)\n",
    "        result = f\"## Pros of AutoGen:\\n{response1.content}\\n\\n## Cons of AutoGen:\\n{response2.content}\\n\\n\"\n",
    "        judgement = f\"{judge}\\n{result}Respond with your decision and brief explanation\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime\n",
    "\n",
    "if ALL_IN_ONE_WORKER:\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "\n",
    "    await Player1Agent.register(worker, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "    await Player2Agent.register(worker, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "else:\n",
    "\n",
    "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker1.start()\n",
    "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "\n",
    "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker2.start()\n",
    "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await worker.send_message(Message(content=\"Go!\"), agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pros of AutoGen:\n",
       "Based on my research, here are the primary advantages and reasons to choose AutoGen for an AI Agent project:\n",
       "\n",
       "*   **Advanced Multi-Agent Collaboration:** AutoGen excels at creating and orchestrating multiple, specialized AI agents that can work together to solve complex tasks. Instead of relying on a single, monolithic LLM, you can build a team of agents (e.g., a \"planner,\" a \"coder,\" a \"critic\") that converse and delegate to achieve a goal more effectively.\n",
       "\n",
       "*   **Flexibility and Customization:** The framework is highly adaptable. You can customize agents for specific roles and capabilities, integrate various LLMs (not just OpenAI's), and equip them with different tools (like code interpreters or external APIs).\n",
       "\n",
       "*   **Human-in-the-Loop Integration:** AutoGen makes it easy to integrate human oversight and feedback directly into the workflow. Agents can pause their work to ask for clarification, approval, or direction, allowing for a blend of autonomous operation and human control.\n",
       "\n",
       "*   **Simplifies Complex Workflows:** It provides a high-level abstraction layer that simplifies the development of sophisticated agent-based applications. It handles the \"plumbing\" of agent conversations and state management, letting developers focus on the logic of the task.\n",
       "\n",
       "*   **Improved Performance and Task Decomposition:** By breaking down a complex problem and assigning parts to specialized agents, AutoGen can often achieve better and more reliable results than a single prompt to one large model.\n",
       "\n",
       "In short, AutoGen is a strong choice if your project can benefit from multiple AI agents collaborating, requires a mix of automation and human oversight, and needs the flexibility to use different models and tools.\n",
       "\n",
       "TERMINATE\n",
       "\n",
       "## Cons of AutoGen:\n",
       "Based on my research, here are the primary disadvantages or \"cons\" of choosing AutoGen for a project:\n",
       "\n",
       "*   **Steep Learning Curve for Complex Scenarios:** While getting a simple two-agent conversation running is straightforward, mastering the framework for complex, customized multi-agent workflows can be difficult. The unique abstractions (like group chats, nested chats, and reply functions) require a significant investment to understand and use effectively.\n",
       "\n",
       "*   **Immature and Evolving Framework:** AutoGen is still under heavy development. This can lead to breaking changes in the API, a lack of stability, and features that may not be fully fleshed out or could be buggy. It may not be as feature-rich as more mature alternatives in certain areas.\n",
       "\n",
       "*   **Documentation and Debugging Challenges:** Users frequently report that the documentation can be hard to navigate, lacks sufficient real-world examples, and may not keep pace with the rapid development. Furthermore, debugging the flow of conversation between multiple agents can be complex and time-consuming.\n",
       "\n",
       "*   **Architectural Constraints:** The framework imposes its own specific architectural patterns. Some users have found these constraints to be limiting for their specific use cases and have pointed out that the API can feel inefficient for certain tasks.TERMINATE\n",
       "\n",
       "\n",
       "\n",
       "## Decision:\n",
       "\n",
       "Based on the provided research, here is the decision and rationale:\n",
       "\n",
       "**Decision:** We will conditionally approve the use of AutoGen for the project.\n",
       "\n",
       "**Rationale:**\n",
       "\n",
       "The decision to use AutoGen is justified if the project's core requirement is solving a complex problem that benefits from the collaboration of multiple, specialized AI agents. The primary \"Pros\" highlight that AutoGen is purpose-built for this exact scenario, offering superior task decomposition and human-in-the-loop capabilities that a single-agent approach would lack.\n",
       "\n",
       "However, this approval is conditional. The project team must be prepared to accept and mitigate the significant risks outlined in the \"Cons.\" Specifically, the project plan must account for a steep learning curve, potential instability due to the framework's immaturity, and the extra time required for debugging complex agent interactions. If the project's requirements are simple and do not necessitate a multi-agent approach, the drawbacks of AutoGen would outweigh its benefits, and it should not be used.\n",
       "\n",
       "In short, we will use AutoGen for its unique strengths in multi-agent orchestration, but only for a problem complex enough to justify the significant development and stability risks involved.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "await worker.stop()\n",
    "if not ALL_IN_ONE_WORKER:\n",
    "    await worker1.stop()\n",
    "    await worker2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "await host.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
