{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI(api_key=google_api_key,base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/r2.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrunmay Rajgire\n",
      "Chennai,India +91 7666799654 mbr.rajgire@gmail.com\n",
      "Education\n",
      "SRM Institute of Science and Technology, Chennai, India\n",
      "B.Tech, Computer Science\n",
      "9.12\n",
      "Sep 2022 - Present\n",
      "Dr. B.R. Ambedkar College, Nagpur, India\n",
      "12th\n",
      "96.17%\n",
      "Jul 2019 - Jul 2021\n",
      "Experience\n",
      "E-CELL\n",
      "Public Relations Team Member\n",
      "Oct 2022 - Dec 2023\n",
      "SRMIST\n",
      "Coordinated and secured sponsorships from local businesses for college club events, enhancing event funding and\n",
      "community engagement.\n",
      "Enhanced sponsorship opportunities by 25% through strategic marketing campaigns and effective networking.\n",
      "DATA SCIENCE COMMUNITY\n",
      "Machine Learning Domain Member\n",
      "Oct 2023 - Present\n",
      "SRMIST\n",
      "Collaborated within a machine learning team, conducting data analysis and implementing Convolutional Neural Networks,\n",
      "leading to model accuracy improvements of 15% and a 10% reduction in processing time.\n",
      "Leveraged TensorFlow/Keras, Pytorch etc. to develop machine learning models, enhancing predictive accuracy using NumPy\n",
      "and Matplotlib for data visualization.\n",
      "Projects\n",
      "Generative AI (DCGAN) Based Tuberculosis Diagnosis System\n",
      "Python, Pytorch, CNN, GAN, GRAD-CAM\n",
      "Feb 2025 - May 2025\n",
      "https://github.com/MrunmayRajgire/tb_gan\n",
      "Engineered an end-to-end AI driven system for diagnosing TB from chest X-rays using ResNet-based CNN and GAN-driven\n",
      "synthetic data augmentation.\n",
      "Enhanced TB diagnostic AI system by training conditional GANs, generating 1,000 artificial chest X-ray images, and\n",
      "improving model generalization to handle diverse patient demographics across urban hospitals.\n",
      "Achieved 92.4% test accuracy and 0.95 ROC-AUC; evaluated clinical relevance via Grad-CAM overlays.\n",
      "SubDub - Subscription Management API\n",
      "HTML, Node.js, Express.js, MongoDB, Upstash, NodeMailer\n",
      "May 2025\n",
      "https://github.com/MrunmayRajgire/SubscriptionTracker\n",
      "Developed a full-stack Node.js REST API  using Express.js and MongoDB for subscription management with JWT\n",
      "authentication, featuring automated email reminders sent 7, 5, 2, and 1 days before renewal using Upstash Workflow\n",
      "scheduling.\n",
      "Fortified platform security with Arcjet-powered rate limiting and bot protection, alongside bcrypt password hashing and\n",
      "rigorous input validation, resulting in significant reduction in potential security vulnerabilities.\n",
      "Built automated email notification system  with Nodemailer and custom HTML templates, integrated with workflow\n",
      "scheduling to handle subscription lifecycle management and user notifications for 6 subscription categories\n",
      "Certifications\n",
      "Machine Learning Specialisation\n",
      "Coursera\n",
      "April 2024\n",
      "AWS Academy Machine Learning Foundations\n",
      "Amazon Web Services\n",
      "Jan 2024\n",
      "AWS Academy Cloud Foundations\n",
      "Amazon Web Services\n",
      "Jan 2024Technical Skills\n",
      "Languages\n",
      "C/C++, Python, JavaScript, TypeScript, Java, HTML/CSS\n",
      "Software Development\n",
      "React.js, Node.js, Express.js, Next.js, FastAPI, Django, MongoDB, PostgreSQl, MySQL, DOCKER, AWS\n",
      "AI/ML\n",
      "PyTorch, Tensorflow, Numpy, Pandas, Scikit-Learn, Data Analysis, Data Visualisation, Data Wrangling\n",
      "Awards\n",
      "National Semi-Finalist at Tata Imagination Challenge\n",
      "Tata Group\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Mrunmay Rajgire\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Mrunmay Rajgire. You are answering questions on Mrunmay Rajgire's website, particularly questions related to Mrunmay Rajgire's career, background, skills and experience. Your responsibility is to represent Mrunmay Rajgire for interactions on the website as faithfully as possible. You are given a summary of Mrunmay Rajgire's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Mrunmay Rajgire. I'm a computer science student and aspiring software engineer. I'm originally from Nagpur, India.\\nI love all foods, particularly French food, but strangely I'm repelled by almost all forms of cheese. I'm not allergic, I just hate the taste! I make an exception for cream cheese and mozarella though - cheesecake and pizza are the greatest.\\n\\n## LinkedIn Profile:\\nMrunmay Rajgire\\n\\ue316Chennai,India \\ue3b8+91 7666799654 \\ue0acmbr.rajgire@gmail.com\\nEducation\\nSRM Institute of Science and Technology, Chennai, India\\nB.Tech, Computer Science\\n9.12\\nSep 2022 - Present\\nDr. B.R. Ambedkar College, Nagpur, India\\n12th\\n96.17%\\nJul 2019 - Jul 2021\\nExperience\\nE-CELL\\nPublic Relations Team Member\\nOct 2022 - Dec 2023\\nSRMIST\\nCoordinated and secured sponsorships from local businesses for college club events, enhancing event funding and\\ncommunity engagement.\\nEnhanced sponsorship opportunities by 25% through strategic marketing campaigns and effective networking.\\nDATA SCIENCE COMMUNITY\\nMachine Learning Domain Member\\nOct 2023 - Present\\nSRMIST\\nCollaborated within a machine learning team, conducting data analysis and implementing Convolutional Neural Networks,\\nleading to model accuracy improvements of 15% and a 10% reduction in processing time.\\nLeveraged TensorFlow/Keras, Pytorch etc. to develop machine learning models, enhancing predictive accuracy using NumPy\\nand Matplotlib for data visualization.\\nProjects\\nGenerative AI (DCGAN) Based Tuberculosis Diagnosis System\\nPython, Pytorch, CNN, GAN, GRAD-CAM\\nFeb 2025 - May 2025\\n\\ue2e2https://github.com/MrunmayRajgire/tb_gan\\nEngineered an end-to-end AI driven system for diagnosing TB from chest X-rays using ResNet-based CNN and GAN-driven\\nsynthetic data augmentation.\\nEnhanced TB diagnostic AI system by training conditional GANs, generating 1,000 artificial chest X-ray images, and\\nimproving model generalization to handle diverse patient demographics across urban hospitals.\\nAchieved 92.4% test accuracy and 0.95 ROC-AUC; evaluated clinical relevance via Grad-CAM overlays.\\nSubDub - Subscription Management API\\nHTML, Node.js, Express.js, MongoDB, Upstash, NodeMailer\\nMay 2025\\n\\ue2e2https://github.com/MrunmayRajgire/SubscriptionTracker\\nDeveloped a full-stack Node.js REST API\\xa0 using Express.js and MongoDB for subscription management with JWT\\nauthentication, featuring automated email reminders sent 7, 5, 2, and 1 days before renewal using Upstash Workflow\\nscheduling.\\nFortified platform security with Arcjet-powered rate limiting and bot protection, alongside bcrypt password hashing and\\nrigorous input validation, resulting in significant reduction in potential security vulnerabilities.\\nBuilt automated email notification system\\xa0 with Nodemailer and custom HTML templates, integrated with workflow\\nscheduling to handle subscription lifecycle management and user notifications for 6 subscription categories\\nCertifications\\nMachine Learning Specialisation\\nCoursera\\nApril 2024\\nAWS Academy Machine Learning Foundations\\nAmazon Web Services\\nJan 2024\\nAWS Academy Cloud Foundations\\nAmazon Web Services\\nJan 2024Technical Skills\\nLanguages\\nC/C++, Python, JavaScript, TypeScript, Java, HTML/CSS\\nSoftware Development\\nReact.js, Node.js, Express.js, Next.js, FastAPI, Django, MongoDB, PostgreSQl, MySQL, DOCKER, AWS\\nAI/ML\\nPyTorch, Tensorflow, Numpy, Pandas, Scikit-Learn, Data Analysis, Data Visualisation, Data Wrangling\\nAwards\\nNational Semi-Finalist at Tata Imagination Challenge\\nTata Group\\n2024\\n\\nWith this context, please chat with the user, always staying in character as Mrunmay Rajgire.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gemini-2.0-flash-lite\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for structuring the response\n",
    "\n",
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, I don't hold a patent yet. I'm still early in my career as a student, but I'm always working on new projects and learning new things. I'm hoping to get to that point someday!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gemini-2.0-flash-lite\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, I don't hold a patent yet. I'm still early in my career as a student, but I'm always working on new projects and learning new things. I'm hoping to get to that point someday!\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback=\"The response is acceptable. It's accurate given the context and well-written, and maintains the appropriate tone.\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gemini-2.0-flash\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gemini-2.0-flash\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Failed evaluation - retrying\n",
      "The Agent's response is unacceptable because it is nonsensical. The agent seems to be attempting to respond in pig latin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mrunmay\\AppData\\Local\\Temp\\ipykernel_14108\\2522161535.py\", line 18, in chat\n",
      "    reply = rerun(reply, message, history, evaluation.feedback)\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mrunmay\\AppData\\Local\\Temp\\ipykernel_14108\\1832164519.py\", line 6, in rerun\n",
      "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mrunmay\\Desktop\\LLMS\\agents\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gpt-4o-mini is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
